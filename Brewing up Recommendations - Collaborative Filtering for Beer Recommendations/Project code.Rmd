---
title: "MGT 8803 Project Code"
author: "Zachary Labkovski"
date: "2023-04-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Code

## Read in Data
```{r}
reviews <- read.csv("beer_reviews.csv")
```

## Define data frame with only data required for matrix
```{r}
review_data <- reviews[c("review_profilename", "beer_beerid", "review_overall")]
```

## Removing NA rows
```{r}
nrow(review_data[(is.na(review_data$review_profilename) | review_data$review_profilename==""), ])
review_data <- review_data[!(review_data$review_profilename==""),]
```

## Remove duplicate ratings (same user, same beer id)
```{r}
library(dplyr)
review_data<-distinct(review_data,beer_beerid,review_profilename,.keep_all = TRUE)
```

## Filter data to beers with at least 500 reviews, users with at least 100 reviews, both to improve model performance and reduce runtime.
```{r}
review_count <- review_data %>% group_by(beer_beerid) %>% summarize(total_beer_review=n())
```
```{r}
user_count <- review_data %>% group_by(review_profilename) %>% summarize(total_user_review=n())
```
```{r}
top_beers <- subset(review_count,review_count$total_beer_review>=500)
top_users <- subset(user_count,user_count$total_user_review>=100)
mydata<-merge(review_data,top_beers,by.x="beer_beerid",by.y="beer_beerid")
mydata<-merge(mydata,top_users,by.x="review_profilename",by.y="review_profilename")
```

## Convert data frame to realRatingMatrix
```{r}
library(recommenderlab)
matrix <- as(mydata[,c(1,2,3)], "realRatingMatrix")
```
```{r}
head(rowCounts(matrix))
```

## Set up evaluation scheme
For evaluation, we'll use 10-fold cross-validation.
```{r}
scheme1 <- evaluationScheme(matrix, method = "split", train = 0.8, k=1, given=-1, goodRating=3.5)
scheme2 <- evaluationScheme(matrix, method = "cross-validation", k=10, given=-1, goodRating=3.5)
```

## Set up model types
```{r}
methods <- list("UBCF Cosine" = list(name="UBCF", param=list(method="cosine")),
                "UBCF Pearson" = list(name="UBCF", param=list(method="pearson")),
                "UBCF Jaccard" = list(name="UBCF", param=list(method="jaccard")), 
                "IBCF Cosine" = list(name="IBCF", param=list(method="cosine")),
                "IBCF Pearson" = list(name="IBCF", param=list(method="pearson")),
                "IBCF Jaccard" = list(name="IBCF", param=list(method="jaccard")))
```

## Model Implementation and Results
```{r}
results1 <- evaluate(scheme1, methods, n = c(1, 3, 5, 10, 15, 20))

```

## Comparison of Evaluation Metrics
```{r}
png('tts_plot.png')
plot(results1, legend = "topleft")
dev.off()
```


# Evaluation using Cross Validation Instead of Train-Test Split


```{r}
results2 <- evaluate(scheme2, methods, n = c(1, 3, 5, 10, 15, 20))
```

## Comparison of Evaluation Metrics
```{r}
png('cv_plot.png')
plot(results2, legend = "topleft")
dev.off()
```

```{r}
results2
```
```{r}
avg(results1)
```

